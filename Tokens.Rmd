---
title: "Tokens"
output: html_document
---

##BPE TOKENIZATION

```{r}
dados = read.csv("poemas.csv")
```

```{r}
autores = c("Fernando Pessoa", "Sophia de Mello Breyner Andresen")
interesse = dados$Author %in% autores
futuro = dados[interesse,]
dados = dados[!interesse, ]


dados$Content -> palavras
palavras = palavras |> tolower() |>
  head(50) |> 
  paste() |>
  gsub(pattern = "\n", replacement = " ") |>
  gsub(pattern = "[^[:alpha:] ]", replacement = "") |> 
  iconv(to = "ASCII//TRANSLIT") |> 
  strsplit(split = " ") |>
  unlist()
palavras = palavras[palavras != ""] |> paste0("_")
#palavras
```

```{r}
get_letter_pairs = function(word, itens) {
  n = nchar(word)
  substrings = list()
  for(i in 1:(n-1)){
    substrings[[i]] = substring(word, 1:(n-i), (i+1):n)
  }
  substrings = unlist(substrings)
  substrings = substrings[substrings %in% itens]
  contagem = table(substrings) |> unlist()
  contagem = list(contagem)
  return(contagem)
}

get_letter_pairs = Vectorize(get_letter_pairs, "word")


get_itens = function(glossario){
  itens = expand.grid(glossario, glossario)
  itens = paste0(itens$Var1, itens$Var2)
  return(itens)
}

criar_glossario = function(corpus, nmax = 100){
  glossario = c(letters, "_")
  
  for(i in 1:nmax){
    print(i)
    itens = get_itens(glossario)
    pares = get_letter_pairs(corpus, itens)
    names(pares) = NULL
    pares = pares |> unlist() |> names() |> table() 
    pares = pares[!(names(pares) %in% glossario)]
    par = pares |> which.max() |> names()
    glossario = c(glossario, par)
  }
  
  return(glossario)
  
}
glossario = criar_glossario(palavras)
```

```{r}
tokenizar = function(palavra, glossario) {
  palavra = paste0(palavra, "_")
  toks = c()
  
  while (nchar(palavra) > 0) {
    encontrado = FALSE
    for (tam in seq(nchar(palavra), 1, -1)) {
      candidato = substr(palavra, 1, tam)
      if (candidato %in% glossario) {
        toks = c(toks, candidato)
        palavra = substr(palavra, tam + 1, nchar(palavra))
        encontrado = TRUE
        break
      }
    }
    if (!encontrado) {
      toks = c(toks, substr(palavra, 1, 1))
      palavra = substr(palavra, 2, nchar(palavra))
    }
  }
  toks
}

tokenizar = Vectorize(tokenizar, "palavra")

tokenizar(c("parente", "semente"), glossario)

```

```{r}
dado_embedding = dados$Content |> tolower() |>
  tail(100) |> 
  paste() |>
  gsub(pattern = "\n", replacement = " ") |>
  gsub(pattern = "[^[:alpha:] ]", replacement = "") |> 
  iconv(to = "ASCII//TRANSLIT") |> 
  strsplit(split = " ") |>
  unlist()
dado_embedding = dado_embedding |> tokenizar(glossario)
```


```{r}
fernando = futuro[futuro$Author == autores[1],]
sophia = futuro[futuro$Author == autores[2], ]
nf = (nrow(fernando)*0.8) |> round()
ns = (nrow(sophia)*0.8) |> round()
idx_f = sample(1:nrow(fernando),nf)
idx_s = sample(1:nrow(sophia), ns)
f_treino = fernando[idx_f,]
f_teste = fernando[-idx_f,]
s_treino = sophia[idx_s,]
s_teste = sophia[-idx_s,]
treino = rbind(f_treino, s_treino)
teste = rbind(f_teste, s_teste)
treino = treino[sample(1:nrow(treino)),]
teste = teste[sample(1:nrow(teste)),]

```

```{r}
limpar_texto = function(texto){
  palavras = texto |> tolower() |>
    gsub(pattern = "\n", replacement = " ") |>
    gsub(pattern = "[^[:alpha:] ]", replacement = "") |> 
    iconv(to = "ASCII//TRANSLIT") |> 
    strsplit(split = " ") |>
    unlist()
  palavras = palavras[palavras != ""]
  return(palavras)
}
limpar_texto = Vectorize(limpar_texto)
```

```{r}
txt_treino = limpar_texto(treino$Content) |> lapply(tokenizar, glossario = glossario)
txt_teste = limpar_texto(teste$Content) |> lapply(tokenizar, glossario = glossario)
```

