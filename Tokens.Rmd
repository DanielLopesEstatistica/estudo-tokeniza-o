---
title: "Tokens"
output: html_document
---

##BPE TOKENIZATION

```{r}
dados = read.csv("poemas.csv")
```

```{r}
autores = c("Fernando Pessoa", "Sophia de Mello Breyner Andresen")
interesse = dados$Author %in% autores
futuro = dados[interesse,]
dados = dados[!interesse, ]


dados$Content -> palavras
palavras = palavras |> tolower() |>
  head(50) |> 
  paste() |>
  gsub(pattern = "\n", replacement = " ") |>
  gsub(pattern = "[^[:alpha:] ]", replacement = "") |> 
  iconv(to = "ASCII//TRANSLIT") |> 
  strsplit(split = " ") |>
  unlist()
palavras = palavras[palavras != ""] |> paste0("_")
#palavras
```

```{r}
get_letter_pairs = function(word, itens) {
  n = nchar(word)
  substrings = list()
  for(i in 1:(n-1)){
    substrings[[i]] = substring(word, 1:(n-i), (i+1):n)
  }
  substrings = unlist(substrings)
  substrings = substrings[substrings %in% itens]
  contagem = table(substrings) |> unlist()
  contagem = list(contagem)
  return(contagem)
}

get_letter_pairs = Vectorize(get_letter_pairs, "word")


get_itens = function(glossario){
  itens = expand.grid(glossario, glossario)
  itens = paste0(itens$Var1, itens$Var2)
  return(itens)
}

criar_glossario = function(corpus, nmax = 100){
  glossario = c(letters, "_")
  
  for(i in 1:nmax){
    print(i)
    itens = get_itens(glossario)
    pares = get_letter_pairs(corpus, itens)
    names(pares) = NULL
    pares = pares |> unlist() |> names() |> table() 
    pares = pares[!(names(pares) %in% glossario)]
    par = pares |> which.max() |> names()
    glossario = c(glossario, par)
  }
  
  return(glossario)
  
}
glossario = criar_glossario(palavras)
```

```{r}
tokenizar = function(palavra, glossario) {
  palavra = paste0(palavra, "_")
  toks = c()
  
  while (nchar(palavra) > 0) {
    encontrado = FALSE
    for (tam in seq(nchar(palavra), 1, -1)) {
      candidato = substr(palavra, 1, tam)
      if (candidato %in% glossario) {
        toks = c(toks, candidato)
        palavra = substr(palavra, tam + 1, nchar(palavra))
        encontrado = TRUE
        break
      }
    }
    if (!encontrado) {
      toks = c(toks, substr(palavra, 1, 1))
      palavra = substr(palavra, 2, nchar(palavra))
    }
  }
  return(toks)
}

tokenizar = Vectorize(tokenizar, "palavra")
```

```{r}
somar_submatriz = function(M, S) {
  lin = intersect(rownames(M), rownames(S))
  col = intersect(colnames(M), colnames(S))
  M[lin, col] = M[lin, col] + S[lin, col]
  return(M)
}

criar_matriz = function(n) {
  i = 1:n
  j = 1:n
  m = outer(i, j, function(i, j) n - abs(i - j))
  return(m)
}

dado_embedding = dados$Content |> tolower() |>
  tail(1000) |> 
  paste() |>
  gsub(pattern = "\n", replacement = " ") |>
  gsub(pattern = "[^[:alpha:] ]", replacement = "") |> 
  iconv(to = "ASCII//TRANSLIT") |> 
  strsplit(split = " ") |>
  unlist()
dado_embedding = dado_embedding |> tokenizar(glossario) |> unlist()

n = length(glossario)
janela = 10
cooc = matrix(0, n, n, dimnames = list(glossario, glossario))
n_dados = length(dado_embedding)

for(i in 1:n_dados){
  ini = max(1, i - janela)
  fim = min(n_dados, i + janela)
  ctx = dado_embedding[ini:fim]
  tamanho_janela = fim-ini+1
  matriz = criar_matriz(tamanho_janela)
  colnames(matriz) = ctx
  rownames(matriz) = ctx
  cooc = somar_submatriz(cooc, matriz)
}

sv = svd(cooc)
emb = sv$u[, 1:100]  
rownames(emb) = glossario
```


```{r}
fernando = futuro[futuro$Author == autores[1],]
sophia = futuro[futuro$Author == autores[2], ]
nf = (nrow(fernando)*0.8) |> round()
ns = (nrow(sophia)*0.8) |> round()
idx_f = sample(1:nrow(fernando),nf)
idx_s = sample(1:nrow(sophia), ns)
f_treino = fernando[idx_f,]
f_teste = fernando[-idx_f,]
s_treino = sophia[idx_s,]
s_teste = sophia[-idx_s,]
treino = rbind(f_treino, s_treino)
teste = rbind(f_teste, s_teste)
treino = treino[sample(1:nrow(treino)),]
teste = teste[sample(1:nrow(teste)),]

```

```{r}
limpar_texto = function(texto){
  palavras = texto |> tolower() |>
    gsub(pattern = "\n", replacement = " ") |>
    gsub(pattern = "[^[:alpha:] ]", replacement = "") |> 
    iconv(to = "ASCII//TRANSLIT") |> 
    strsplit(split = " ") |>
    unlist()
  palavras = palavras[palavras != ""]
  return(palavras)
}
limpar_texto = Vectorize(limpar_texto)
```

```{r}
txt_treino = limpar_texto(treino$Content) |> lapply(tokenizar, glossario = glossario) |> unname()

txt_teste = limpar_texto(teste$Content) |> lapply(tokenizar, glossario = glossario) |> unname()
```

```{r}
embedding_lista = function(M, L) {
  res = lapply(L, function(v) {
    v = unlist(v)
    if (is.character(v)) v = match(v, rownames(M))
    cm = colMeans(M[v, , drop=FALSE])
    unname(cm) 
  })
  res = do.call(rbind, res)
  colnames(res) = colnames(M)
  rownames(res) = names(L)
  return(res)
}

emb_treino = embedding_lista(emb, txt_treino)
emb_treino[is.nan(emb_treino)] = 0
emb_teste = embedding_lista(emb, txt_teste)
emb_teste[is.nan(emb_teste)] = 0
```

## 1 = fernando pessoa
## 2 = sophia de mello

```{r}
library(caret)
library(xgboost)

y_treino = treino$Author |> as.factor() |> as.numeric() 
y_treino = y_treino - 1
y_teste = teste$Author |> as.factor() |> as.numeric()
y_teste = y_teste - 1
dtreino = xgb.DMatrix(data = emb_treino, label = y_treino)
dteste = xgb.DMatrix(data = emb_teste, label = y_teste)

if(!exists("modelo_poemas", envir = .GlobalEnv)){
  modelo_poemas = xgboost(
    data = dtreino,
    objective = "multi:softprob", 
    num_class = 2,
    nrounds = 100,
    verbose = 0
  )
}
```

```{r}
library(dplyr) 

predizer = function(modelo, dados){
  predicoes = predict(modelo, dados, reshape = TRUE) |> as.data.frame()
  predicoes = predicoes %>% 
    mutate(num_predicao = max.col(., ties.method = "first") - 1)
  return(predicoes)
}

predicoes <- predizer(modelo_poemas, emb_teste)

confusao = table(predicoes$num_predicao, y_teste)

acuracia = sum(diag(confusao))/sum(confusao)
acuracia
confusao
```

